{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fa403ce",
   "metadata": {},
   "source": [
    "# NVIDIA Time slicing deployment vs regular Instance deployment.\n",
    "\n",
    "### Objective\n",
    "- This analysis aims to compare the performance of two vLLM deployment strategies on a single GPU.\n",
    "\n",
    "### Experiment\n",
    "[0] Baseline Configuration: This is a standard deployment where a single vLLM instance runs in one container. This baseline utilizes approximately 20 GB of VRAM and has dedicated access to all 98 SMs.\n",
    "<br><br>[1] Time-Sliced Configuration: This setup uses NVIDIA's Time-Slicing feature to deploy two vLLM instances on one GPU. Each instance is allocated approximately 20 GB of VRAM, and both instances share the GPU's 98 Streaming Multiprocessors (SMs).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c84369d",
   "metadata": {},
   "source": [
    "### Methodology\n",
    "To simulate a realistic workload and minimize the pre-fill advantages of the vLLM KV Cache, the test will use an Indonesian chat dataset to generate requests.\n",
    "The dataset will be pre-processed by removing the final \"assistant\" message from each conversation. This effectively creates a real-world prompt, requiring the model to generate the next response in the dialogue.\n",
    "\n",
    "Dataset:\n",
    "https://huggingface.co/datasets/IzzulGod/indonesian-conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c22771",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
